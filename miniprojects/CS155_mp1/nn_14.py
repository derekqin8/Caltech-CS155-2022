# -*- coding: utf-8 -*-
"""nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1io3DED1C023BXFfVi3fYspqwre321JGh

# Set Up
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt
import seaborn as sns

import sklearn

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Read in Data"""

train_df = pd.read_csv("https://raw.githubusercontent.com/derekqin8/Caltech-CS155-2022/main/mps/CS155_mp1/data/LOANS_TRAIN.csv")
test_df = pd.read_csv("https://raw.githubusercontent.com/derekqin8/Caltech-CS155-2022/main/mps/CS155_mp1/data/LOANS_TEST.csv")

train_df.head()

train_df.info()

"""# Preprocess Data"""

from sklearn.impute import SimpleImputer

from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder

def preprocess(train_df, test_df):
    ndf = pd.concat([train_df, test_df], axis=0)

    ord_enc = OrdinalEncoder()
    lbl_enc = LabelEncoder()
    scaler = StandardScaler()
    # id: drop
    ndf.drop('id', axis=1, inplace=True)
    # loan_amnt:
    ndf['loan_amnt'] = (ndf['loan_amnt'] - ndf['loan_amnt'].mean()) / ndf['loan_amnt'].std()
    # term_(months):
    ndf['36_months'] = (ndf['term_(months)'] == 36)
    ndf.drop('term_(months)', axis=1, inplace=True)
    # int_rate:
    ndf['int_rate'] = ndf['int_rate'].str.rstrip('%').apply(float)
    ndf['int_rate'] = (ndf['int_rate'] - ndf['int_rate'].mean()) / ndf['int_rate'].std()
    # installment: no need for processing
    ndf['installment'] = (ndf['installment'] - ndf['installment'].mean()) / ndf['installment'].std()
    # grade:
    ndf['grade'] = ord_enc.fit_transform(ndf[['grade']]) + 1
    # sub_grade:
    ndf['sub_grade'] = ord_enc.fit_transform(ndf[['sub_grade']]) + 1
    # emp_title: not useful
    ndf.drop('emp_title', axis=1, inplace=True)
    # emp_length:
    # ndf['emp_length_num'] = ndf["emp_length"].str.replace("year","", regex=True)
    # ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('s', '', regex=True)
    # ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('10', '15', regex=True)
    # ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('+', '', regex=True)
    # ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('< 1', '0', regex=True)
    # ndf['emp_length_num'] = ndf['emp_length_num'].fillna('-1')

    # ndf['emp_length_num'] = ndf['emp_length_num'].apply(int)

    ndf.drop('emp_length', axis=1, inplace=True)
    # home_ownership:
    ndf = pd.get_dummies(ndf, prefix='ho', columns=['home_ownership'])
    # annual_inc:
    ndf['annual_inc'] = (ndf['annual_inc'] - ndf['annual_inc'].mean()) / ndf['annual_inc'].std()
    # verification_status:
    ndf = pd.get_dummies(ndf, prefix='var', columns=['verification_status'])
    # issue_d:
    # ndf['issue_yr'] = ndf['issue_d'].str[-4:].apply(int)
    ndf.drop('issue_d', axis=1, inplace=True)
    # purpose:
    ndf = pd.get_dummies(ndf, prefix='purp', columns=['purpose'])
    # title: not useful
    ndf.drop('title', axis=1, inplace=True)
    # zip_code: not useful
    ndf.drop('zip_code', axis=1, inplace=True)
    # addr_state:
    # ndf['addr_state'] = lbl_enc.fit_transform(df[['addr_state']])
    # ndf = pd.get_dummies(ndf, prefix='state', columns=['addr_state'])
    ndf.drop('addr_state', axis=1, inplace=True)

    # dti:
    ndf['dti'] = (ndf['dti'] - ndf['dti'].mean()) / ndf['dti'].std()
    # earliest_cr_line: too hard to process :(
    ndf.drop('earliest_cr_line', axis=1, inplace=True)
    # open_acc:
    # pub_rec:
    # revol_bal:
    ndf['revol_bal'] = (ndf['revol_bal'] - ndf['revol_bal'].mean()) / ndf['revol_bal'].std()
    # revol_util:
    ndf['revol_util'] = ndf['revol_util'].str.rstrip('%').apply(float)
    ndf['revol_util'] = (ndf['revol_util'] - ndf['revol_util'].mean()) / ndf['revol_util'].std()
    # tot_acc:
    # initial_list_status:
    ndf['initial_list_status'] = (ndf['initial_list_status'] == 'f')
    # application_type: not useful
    ndf.drop('application_type', axis=1, inplace=True)
    # mort_acc:
    ndf['mort_acc'] = ndf['mort_acc'].fillna(0)
    # pub_rec_bankruptcies:

    X_train = ndf[:train_df.shape[0]].copy()
    X_train['loan_status'] = (X_train['loan_status'] == 'Charged Off')
    X_train[X_train.isnull()] = 0.0

    X_test = ndf[-test_df.shape[0]:].copy()
    X_test.drop('loan_status', axis=1, inplace=True)
    X_test[X_test.isnull()] = 0

    return X_train, X_test

train, test = preprocess(train_df, test_df)

train

test

train.info()

"""# Model Fit

## Neural Network
"""

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from sklearn.metrics import roc_auc_score

X = train.drop('loan_status', axis=1)
y = train['loan_status']

ntrain = round(0.9*X.values.shape[0])
ntest = X.values.shape[0] - ntrain
print(ntrain)
print(ntest)
batch_size=1024
X_np = torch.from_numpy(np.vstack(X.values).astype(float)).float()
y_np = torch.from_numpy(y.values.astype(float)).float().reshape((y.values.shape[0], 1))

test_np = torch.from_numpy(np.vstack(test.values).astype(float)).float()

print(X_np.shape)
print(y_np.shape)

# full_ds = torch.utils.data.TensorDataset(X_np, y_np)

# [train_ds, cv_ds] = torch.utils.data.random_split(full_ds, [ntrain, ntest])

X_train = X_np[:ntrain]
y_train = y_np[:ntrain]

X_cv = X_np[ntrain:]
y_cv = y_np[ntrain:]

train_ds = torch.utils.data.TensorDataset(X_train, y_train)
cv_ds = torch.utils.data.TensorDataset(X_cv, y_cv)

train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)
cv_loader = torch.utils.data.DataLoader(cv_ds, batch_size=batch_size, shuffle=False)

p = 0.2
from torch.nn.modules.activation import Softmax
model = nn.Sequential(
    nn.Linear(X_np.shape[1], 400),
    nn.BatchNorm1d(400),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(400, 500),
    nn.BatchNorm1d(500),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(500, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),
    
    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 1000),
    nn.BatchNorm1d(1000),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(1000, 600),
    nn.BatchNorm1d(600),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(600, 400),
    nn.BatchNorm1d(400),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(400, 200),
    nn.BatchNorm1d(200),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(200, 100),
    nn.BatchNorm1d(100),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(100, 50),
    nn.BatchNorm1d(50),
    nn.LeakyReLU(),
    nn.Dropout(p),

    nn.Linear(50, 1),
    nn.Sigmoid()
)
model = model.cuda()

optimizer=torch.optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.BCELoss()

train_score = []
cv_score = []

for epoch in range(40):
    model.train()
    epoch_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.cuda(), target.cuda()
        # Erase accumulated gradients
        optimizer.zero_grad()

        # Forward pass
        output = model(data)

        # Calculate loss
        loss = loss_fn(output, target)

        epoch_loss += loss.item()

        # Backward pass
        loss.backward()
        
        # Weight update
        optimizer.step()

    # Track loss each epoch
    print()

    model.eval()

    with torch.no_grad():
        train_probs = model(X_train.cuda()).cpu()

        train_auc = roc_auc_score(y_train, train_probs)
        train_score.append(train_auc)

        print('Train Epoch:', epoch + 1, 'Loss:', epoch_loss / ntrain, 'AUC:', train_auc)

        test_loss = 0

        for data, target in cv_loader:
            data, target = data.cuda(), target.cuda()
            output = model(data)
            test_loss += loss_fn(output, target).item()

        test_loss /= len(cv_loader.dataset)

        test_probs = model(X_cv.cuda()).cpu()

        test_auc = roc_auc_score(y_cv, test_probs)
        cv_score.append(test_auc)

        print('Test Epoch:', epoch + 1, 'Loss:', test_loss, 'AUC:', test_auc)

import matplotlib.pyplot as plt
plt.plot(range(len(train_score)), train_score)
plt.plot(range(len(cv_score)), cv_score)

model.eval()

with torch.no_grad():
    train_probs = model(X_np.cuda())
    print(roc_auc_score(y_np, train_probs.cpu()))
    # train_probs = model(X_np)
    # print(roc_auc_score(y_np, train_probs))

probs = model(test_np.cuda()).detach().cpu().numpy().reshape(-1)

print(probs)
print(probs.shape)

submission = pd.DataFrame({'Id': test_df['id'], 'loan_status': probs})
# you could use any filename. We choose submission here
submission.to_csv('submission.csv', index=False)

submission.head()