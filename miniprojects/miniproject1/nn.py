# -*- coding: utf-8 -*-
"""nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1io3DED1C023BXFfVi3fYspqwre321JGh

# Set Up
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt
import seaborn as sns

import sklearn

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Read in Data"""

train_df = pd.read_csv("https://raw.githubusercontent.com/derekqin8/Caltech-CS155-2022/main/mps/CS155_mp1/data/LOANS_TRAIN.csv")
test_df = pd.read_csv("https://raw.githubusercontent.com/derekqin8/Caltech-CS155-2022/main/mps/CS155_mp1/data/LOANS_TEST.csv")

train_df.head()

train_df.info()

from sklearn.impute import SimpleImputer

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder

def preprocess(train_df, test_df):
    ndf = pd.concat([train_df, test_df], axis=0)

    ord_enc = OrdinalEncoder()
    lbl_enc = LabelEncoder()

    # loan_amnt: no need for processing
    # term_(months):
    ndf['36_months'] = (ndf['term_(months)'] == 36)
    ndf.drop('term_(months)', axis=1, inplace=True)
    # int_rate:
    ndf['int_rate'] = ndf['int_rate'].str.rstrip('%').apply(float)
    # installment: no need for processing
    # grade:
    ndf['grade'] = ord_enc.fit_transform(ndf[['grade']])
    # sub_grade:
    ndf['sub_grade'] = ord_enc.fit_transform(ndf[['sub_grade']])
    # emp_title: not useful
    ndf.drop('emp_title', axis=1, inplace=True)
    # emp_length:
    ndf['emp_length_num'] = ndf["emp_length"].str.replace("year","", regex=True)
    ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('s', '', regex=True)
    ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('+', '', regex=True)
    ndf['emp_length_num'] = ndf['emp_length_num'].str.replace('< 1', '0', regex=True)
    ndf['emp_length_num'] = ndf['emp_length_num'].fillna('-1')

    ndf['emp_length_num'] = ndf['emp_length_num'].apply(int)

    ndf.drop('emp_length', axis=1, inplace=True)
    # home_ownership:
    ndf = pd.get_dummies(ndf, prefix='ho', columns=['home_ownership'])
    # annual_inc: no need for processing
    # verification_status:
    ndf = pd.get_dummies(ndf, prefix='var', columns=['verification_status'])
    # issue_d: not useful
    ndf.drop('issue_d', axis=1, inplace=True)
    # purpose:
    ndf = pd.get_dummies(ndf, prefix='purp', columns=['purpose'])
    # title: not useful
    ndf.drop('title', axis=1, inplace=True)
    # zip_code: not useful
    ndf.drop('zip_code', axis=1, inplace=True)
    # addr_state:
    # ndf['addr_state'] = lbl_enc.fit_transform(df[['addr_state']])
    ndf.drop('addr_state', axis=1, inplace=True)

    # dti: no need for processing
    # earliest_cr_line: too hard to process :(
    ndf.drop('earliest_cr_line', axis=1, inplace=True)
    # open_acc: no need for processing
    # pub_rec: no need for processing
    # revol_bal: no need for processing
    # revol_util:
    ndf['revol_util'] = ndf['revol_util'].str.rstrip('%').apply(float)
    # tot_acc: no need for processing
    # initial_list_status:
    ndf['initial_list_status'] = (ndf['initial_list_status'] == 'f')
    # application_type: not useful
    ndf.drop('application_type', axis=1, inplace=True)
    # mort_acc:
    ndf['mort_acc'] = ndf['mort_acc'].fillna(0)
    # pub_rec_bankruptcies: no need for processing

    X_train = ndf[:train_df.shape[0]].copy()
    X_train['loan_status'] = (X_train['loan_status'] == 'Charged Off')
    X_train[X_train.isnull()] = 0

    X_test = ndf[-test_df.shape[0]:].copy()
    X_test.drop('loan_status', axis=1, inplace=True)
    X_test[X_test.isnull()] = 0

    return X_train, X_test

train, test = preprocess(train_df, test_df)

train

test

"""# Model Fit

## Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

from sklearn.metrics import roc_auc_score

n_estimators = 500
clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=10, class_weight={1: 5, 0: 1})
X = train.drop('loan_status', axis=1)
y = train['loan_status']

clf.fit(X, y)

scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')
print(scores)
print(sum(scores)/len(scores))

train_probs = clf.predict_proba(X)[:,1]
print(roc_auc_score(y, train_probs))

probs = clf.predict_proba(test)
charged_off_probs = probs[:, 1]

print(probs)
print(probs.shape)

submission = pd.DataFrame({'Id': test_df['id'], 'loan_status': charged_off_probs})
# you could use any filename. We choose submission here
submission.to_csv('submission.csv', index=False)

submission.head()

# """## AdaBoost"""

# from sklearn.ensemble import AdaBoostClassifier
# from sklearn.model_selection import cross_val_score
# from sklearn.preprocessing import StandardScaler
# from sklearn.pipeline import make_pipeline

# from sklearn.metrics import roc_auc_score

# n_estimators = 500
# clf = make_pipeline(StandardScaler(), AdaBoostClassifier(n_estimators=n_estimators))
# X = train.drop('loan_status', axis=1)
# y = train['loan_status']

# clf.fit(X, y)

# scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')
# print(scores)
# print(sum(scores)/len(scores))

# probs = clf.predict_proba(test)
# charged_off_probs = probs[:, 1]

# submission = pd.DataFrame({'Id': test_df['id'], 'loan_status': charged_off_probs})
# # you could use any filename. We choose submission here
# submission.to_csv('submission.csv', index=False)

# submission.head()

# """## SVC"""

# from sklearn.svm import SVC
# from sklearn.ensemble import BaggingClassifier
# from sklearn.preprocessing import StandardScaler
# from sklearn.pipeline import make_pipeline

# from sklearn.model_selection import cross_val_score

# from sklearn.metrics import roc_auc_score

# n_estimators=1000
# clf = make_pipeline(StandardScaler(), BaggingClassifier(SVC(kernel='linear', max_iter=100), n_estimators=n_estimators, max_samples=1.0/n_estimators, max_features=5))
# X = train.drop('loan_status', axis=1)
# y = train['loan_status']

# clf.fit(X, y)

# # scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')
# # print(scores)
# # print(sum(scores)/len(scores))

# train_probs = clf.predict_proba(X)[:,1]
# print(roc_auc_score(y, train_probs))

# probs = clf.predict_proba(test)
# charged_off_probs = probs[:, 1]

# submission = pd.DataFrame({'Id': test_df['id'], 'loan_status': charged_off_probs})
# # you could use any filename. We choose submission here
# submission.to_csv('submission.csv', index=False)

# submission.head()